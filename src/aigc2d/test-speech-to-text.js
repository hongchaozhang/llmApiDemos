// Speech to Text (STT) test with AIGC2D Whisper
const AIGC2DClient = require('./aigc2d-client');
const config = require('./config');
const fs = require('fs-extra');
const path = require('path');

async function testSpeechToText() {
  console.log('ğŸ¤ æµ‹è¯•è¯­éŸ³è¯†åˆ«(STT)åŠŸèƒ½...\n');
  
  const client = new AIGC2DClient();
  console.log('ğŸŒ ä½¿ç”¨æ¥å£åœ°å€:', client.getCurrentURL());
  
  const outputDir = path.join(__dirname, 'outputs');
  
  // æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
  const audioFiles = [
    { filename: 'hello-chinese.mp3', expectedLanguage: 'zh' },
    { filename: 'hello-english.mp3', expectedLanguage: 'en' }
  ];
  
  console.log('ğŸ” æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶...');
  
  for (const audioInfo of audioFiles) {
    const audioPath = path.join(outputDir, audioInfo.filename);
    
    if (await fs.pathExists(audioPath)) {
      console.log(`ğŸ“ æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶: ${audioInfo.filename}`);
      
      try {
        console.log(`ğŸ§ æ­£åœ¨è¯†åˆ«è¯­éŸ³å†…å®¹...`);
        
        const startTime = Date.now();
        
        // åˆ›å»ºæ–‡ä»¶æµ
        const audioBuffer = await fs.readFile(audioPath);
        const audioFile = new File([audioBuffer], audioInfo.filename, {
          type: 'audio/mpeg'
        });
        
        const response = await client.speechToText(audioFile, {
          model: config.models.whisper,
          language: audioInfo.expectedLanguage,
          response_format: 'json',
          temperature: '0.0'
        });
        
        const endTime = Date.now();
        
        console.log('âœ… è¯­éŸ³è¯†åˆ«æˆåŠŸï¼');
        console.log(`ğŸ“ è¯†åˆ«ç»“æœ: "${response.text}"`);
        console.log(`ğŸŒ æ£€æµ‹è¯­è¨€: ${response.language || audioInfo.expectedLanguage}`);
        console.log(`â±ï¸  è¯†åˆ«è€—æ—¶: ${endTime - startTime}ms\n`);
        
      } catch (error) {
        console.error(`âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥ (${audioInfo.filename}):`, error.message);
        
        if (error.message.includes('model')) {
          console.log('ğŸ’¡ æç¤º: å¯èƒ½Whisperæ¨¡å‹ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥AIGC2Dæ”¯æŒçš„æ¨¡å‹åˆ—è¡¨\n');
        } else if (error.message.includes('file')) {
          console.log('ğŸ’¡ æç¤º: éŸ³é¢‘æ–‡ä»¶æ ¼å¼å¯èƒ½ä¸æ”¯æŒï¼Œè¯·å°è¯•å…¶ä»–æ ¼å¼\n');
        }
      }
    } else {
      console.log(`âš ï¸  æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶: ${audioInfo.filename}`);
      console.log('ğŸ’¡ è¯·å…ˆè¿è¡Œ yarn test:tts ç”Ÿæˆæµ‹è¯•éŸ³é¢‘æ–‡ä»¶\n');
    }
    
    // é¿å…é¢‘ç‡é™åˆ¶
    if (audioFiles.indexOf(audioInfo) < audioFiles.length - 1) {
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
  }
  
  // æ¼”ç¤ºè‡ªå®šä¹‰éŸ³é¢‘è¯†åˆ«
  console.log('ğŸ“‹ è¯­éŸ³è¯†åˆ«åŠŸèƒ½è¯´æ˜ï¼š');
  console.log('- æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼: mp3, wav, m4a, webmç­‰');
  console.log('- æ”¯æŒå¤šè¯­è¨€è¯†åˆ«ï¼ŒåŒ…æ‹¬ä¸­æ–‡å’Œè‹±æ–‡');
  console.log('- ä½¿ç”¨Whisperæ¨¡å‹ï¼Œå…·æœ‰é«˜å‡†ç¡®ç‡');
  console.log('- å¯ä»¥å¤„ç†é•¿è¾¾25MBçš„éŸ³é¢‘æ–‡ä»¶');
  
  console.log('\nğŸ¤ è¯­éŸ³è¯†åˆ«æµ‹è¯•å®Œæˆï¼');
  console.log('ğŸ’¡ è¦æµ‹è¯•è‡ªå®šä¹‰éŸ³é¢‘æ–‡ä»¶ï¼Œè¯·å°†æ–‡ä»¶æ”¾å…¥outputsæ–‡ä»¶å¤¹å¹¶ä¿®æ”¹ä»£ç ä¸­çš„æ–‡ä»¶åã€‚');
}

if (require.main === module) {
  testSpeechToText();
}

module.exports = testSpeechToText;
